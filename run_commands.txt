pip install -r requirements.txt

python prepare_data.py download

python tokenizer.py train_vocab --vocab_size=512 --dtype=float32

python check_tokenization.py --dtype=float32

python tokenizer.py pretokenize --vocab_size=512 --dtype=float32

python train.py --dtype=float16 --test_run --lr=1e-4

#Max files to train is 30 on Google Colab's Tesla T4 GPU, with an increased learning rate
python train.py --dtype=float16 --max_files=30 --max_iters=5000 --lr=5e-4

python inference.py --model_path "models/llama3_float16_20250202_174405/final_model.pt" --dtype=float32 --max_gen_len=250 --prompt "Once upon a time"